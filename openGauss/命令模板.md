# SQL模板

~~~shell
#1、单机执行 
echo `date "+%Y-%m-%d %H:%M:%S"` >> Result.txt
echo `hostname -i|awk '{print $1}'` >> Result.txt
gsql -d postgres -p 5432 -c " select version(); " >> Result.txt

#2、批量执行 #说明：前提是跳板机可以直接ssh到各个数据库服务器 
#说明：数据库的端口都定义在环境变量里面（否则需要-p指定数据库端口） 
#说明：ip.txt存放IP信息 
#说明：query.sql存放查询SQL 
#说明：Result.txt存放查询结果 
for i in $(cat ip.txt);do 
   scp query.sql omm@$i:/home/omm
   echo $i >> Result.txt
   echo `date "+%Y-%m-%d %H:%M:%S"` >> Result.txt
   ssh omm@$i "gsql -d postgres -f /home/omm/query.sql" >> Result.txt
   ssh omm@$i "rm /home/omm/query.sql"
done
~~~

# OS模板

~~~shell
for i in $(cat ip.txt);do
	echo $i >> Result.txt
	echo `date "+%Y-%m-%d %H:%M:%S"` >> Result.txt
	ssh omm@$i "/bin/ls -lrth /opengauss/gaussdb_log/omm/dn_6001" >> Result.txt
done
~~~



# 并行查询

~~~shell
# 并行查询集群状态
#!/bin/sh
date
ips="ip1"
cur_time=`date +"%Y_%m_%d_%H_%M_%S_%s"`

for i in $(cat ${ips})
do
    (
    #echo $i
    ssh $i "source ~/.bashrc;gs_om -t query" > /tmp/${i}_${cur_time}_${RANDOM}
    )&
done
wait
cat  /tmp/*_${cur_time}_* > all_${cur_time}.txt
~~~



# 批量执行命令

~~~shell
gs_ssh -c "gaussdb -V"
gs_ssh -c "gs_om -V"
gs_ssh -c "gsql -V"
gs_ssh -c "cm_ctl -V"
gs_ssh -c "gsql -p 12345 -d postgres -c \"select version();\""
gs_ssh -c "gsql -p 12345 -d postgres -c 'show upgrade_mode;'"
gs_ssh -c "gsql -p 12345 -d postgres -c \"select working_version_num();\""
~~~

# 报错排查关键字

~~~shell
#查看报错关键字的前后5行信息
grep -inE -C 5 "fatal|error" *.log
~~~

# 检查互信

~~~shell
#获取IP和主机名信息（三节点都执行）
#方式一（gs_om集群信息中获取）
HostIp_lists=`gs_om -t status --detail --time-out=1|awk -v ORS=' ' '(NR>=5&&NR<=7){print $2,$3}'`
echo $HostIp_lists > /tmp/check_ssh.txt
cat /tmp/check_ssh.txt

#方式二（xml文件中获取）
#以下是xml中主机名和IP信息部分
<PARAM value="192.168.1.10,192.168.1.11,192.168.1.12" name="backIp1s"/>
<PARAM value="testosa,testosb,testosc" name="nodeNames"/>
<PARAM name="nodeNames" value="testosa,testosb,testosc" />
<PARAM name="backIp1s" value="192.168.1.10,192.168.1.11,192.168.1.12"/>

grep -E "nodeNames|backIp1s" 2standby.xml|grep -oE "value=\".*"|awk -F '"' '{print $2}'|awk -v FS=',' -v OFS=' ' '{print $1,$2,$3}'|awk -v RS=' ' '{print}' > /tmp/check_ssh.txt
grep -E "nodeNames|backIp1s" 2standby.xml|grep -oE "value=\".*"|awk -F '"' '{print $2}'|awk -v FS=',' -v OFS=' ' -v ORS=' ' '{print $1,$2,$3}' > /tmp/check_ssh.txt
grep -E "nodeNames|backIp1s" 1.xml|grep -oE "value=\".*"|awk -F '"' '{print $2}' |tr ',' ' '|xargs -n1 > /tmp/check_ssh.txt
grep -E "nodeNames|backIp1s" 1.xml|grep -oE "value=\".*"|cut -d \" -f 2|tr ',' ' '|xargs -n1 > /tmp/check_ssh.txt


#方式三（gs_om -t view）
gs_om -t view|grep -E "nodeName|datanodeListenIP"|awk -F ':' '{print $2}' > /tmp/check_ssh.txt


#root用户和omm用户互信检查（三节点都执行）
for i in `cat /tmp/check_ssh.txt`;do
ssh $i hostname
done
~~~

# sql耗时分布统计

~~~shell
#!/bin/bash
source /home/omm/.bashrc
read -p "Please input Datetime [example: '2022-11-22 11:00:00']: " Datetime0
Datetime1="timestamp$Datetime0 + interval '10 second'"
Datetime2="timestamp$Datetime0 + interval '20 second'"
Datetime3="timestamp$Datetime0 + interval '30 second'"
Datetime4="timestamp$Datetime0 + interval '40 second'"
Datetime5="timestamp$Datetime0 + interval '50 second'"
Datetime6="timestamp$Datetime0 + interval '60 second'" 
gsql -p 12345 -d postgres << !
select 
  sum(case when db_time < 20 * 1000 then 1 else 0 end) as "(0,20)", 
  sum(case when db_time >= 20 * 1000 and db_time < 40 * 1000 then 1 else 0 end) as "[20,40)", 
  sum(case when db_time >= 40 * 1000 and db_time < 60 * 1000 then 1 else 0 end) as "[40,60)", 
  sum(case when db_time >= 60 * 1000 and db_time < 80 * 1000 then 1 else 0 end) as "[60,80)", 
  sum(case when db_time >= 80 * 1000 and db_time < 100 * 1000 then 1 else 0 end) as "[80,100)", 
  sum(case when db_time >= 100 * 1000 and db_time < 120 * 1000 then 1 else 0 end) as "[100,120)", 
  sum(case when db_time >= 120 * 1000 and db_time < 140 * 1000 then 1 else 0 end) as "[120,140)", 
  sum(case when db_time >= 140 * 1000 and db_time < 160 * 1000 then 1 else 0 end) as "[140,160)", 
  sum(case when db_time >= 160 * 1000 and db_time < 180 * 1000 then 1 else 0 end) as "[160,180)", 
  sum(case when db_time >= 180 * 1000 and db_time < 200 * 1000 then 1 else 0 end) as "[180,200)", 
  sum(case when db_time >= 200 * 1000 then 1 else 0 end) as "[200,+∞)" 
from 
  statement_history 
where 
  1 = 1 
  and db_name = 'bjmdb' 
  and query ~ 'bmsql_' 
  and start_time >= $Datetime0 
  and finish_time <= $Datetime1;
!
~~~

# 监控磁盘和IO

~~~shell
#!/bin/bash
date '+%Y-%m-%d %H:%M:%S' >>io_mem.log
echo "----------------------------------------------------------------------------------" >>io_mem.log
echo "IO info" >>io_mem.log
echo "----------------------------------------------------------------------------------" >>io_mem.log
iostat -xm|grep -E "Device|sdc" >>io_mem.log
echo "----------------------------------------------------------------------------------" >>io_mem.log
echo "memory info" >>io_mem.log
echo "----------------------------------------------------------------------------------" >>io_mem.log
free -k >>io_mem.log
echo "----------------------------------------------------------------------------------" >>io_mem.log
#查看IO信息
awk '/2022-11-22 18:47:56/,/2022-11-22 18:48:16/ {print }' io_mem.log|grep -E "2022-|sdc"|sed '{N;s/\n/ /}'
#查看内存信息
awk '/2022-11-22 18:47:56/,/2022-11-22 18:48:16/ {print }' io_mem.log|grep -E "2022-|Mem"|sed '{N;s/\n/ /}'
~~~



# 创建大量表

~~~shell
for i in {1..10}
do
    (
    gsql -d testdb -p 12345 -c "
	create table employees_$i
    ( employee_id    serial    primary key
    , first_name     varchar2(20)
    , last_name      varchar2(25) constraint     emp_last_name_nn  not null
    , email          varchar2(25) constraint     emp_email_nn  not null
    , phone_number   varchar2(20)
    , hire_date      date         constraint     emp_hire_date_nn  not null
    , job_id         int8         constraint     emp_job_nn  not null
    , salary         number(8,2)
    , commission_pct number(4,2)
    , manager_id     int8
    , department_id  int8
    , constraint     emp_salary_min check (salary > 0) 
    --, constraint     emp_email_uk unique (email)
    ) ;
	insert into employees_$i
select generate_series(1,10) as key,
       substr(md5(random()::text),2,5),
       substr(md5(random()::text),2,8),
	   substr(md5(random()::text),2,5)||'@163.com',
	   concat('1',ceiling(random()*9000000000+1000000000)),
	   (random()*(2022-1990)+1990)::int||'-'||(random()*(12-1)+1)::int||'-'||(random()*(28-1)+1)::int,
	   (random()*(50-10)+10)::int,
	   (random()*(10000-3000)+3000)::number(8,2),
	   (random()*(1-0)+0)::number(4,2),
	   (random()*(100-1)+1)::int,
	   (random()*(10-1)+1)::int;
	"
	sleep 1
    )&
done
~~~

